\documentclass[french]{article}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{amsmath}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\ra{1.3}

\title{Hands-On Machine Learning}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  frame=leftline,
  rulecolor=\color{gray},
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a }}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}

\begin{document}
\date{}

\maketitle

\setlength{\parindent}{0cm}

\section{Extract the data}

Get a quick description of the data (number of rows and each attribute's type):
\begin{lstlisting}
  df.info()
\end{lstlisting}

Find out what categories exist:
\begin{lstlisting}
  df['c'].value_counts()
\end{lstlisting}

Summary of the numerical attributes:
\begin{lstlisting}
  df.describe()
\end{lstlisting}

Plot histogram for each numerical attribute:
\begin{lstlisting}
  %matplotlib inline
  import matplotlib.pyplot as plt
  df.hist(bins=50)
  plt.show()
\end{lstlisting}
Rq. Calling \verb|show()| in a Jupyter notebook is optional


\subsection{Test set}

Create a test set:
\begin{lstlisting}
  def split_train_set(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffle_indices[test_set_size:]

    return data.iloc[train_indices], data.iloc[test_indices]
\end{lstlisting}

Rq. If the program runs again, the test set will not be the same which is a problem.\\
One solution is to save the test set. Another option is to set the random number generator's seed. Both solutions will break next time you fetch an updated dataset. A common solution is to use each instance's identifier (unique and immutable) to decide whether or not it should go in the test set. For example, you could compute a hash of each instance's identifier and put that instance in the test set if the hash is lower or equal to 20\%
\begin{lstlisting}
  from zlib import crc32

  def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

  def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))

    return data.loc[~in_test_set], data.loc[in_test_set]
\end{lstlisting}

Rq. If you use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset, and no row ever gets deleted. If this is not possible, then you can try to use the most stable featues to build a unique identifier.\\

Scikit-learn provides a few function to split datasets. \\The simplest is \verb|split_train_test|, which does pretty much the same thing as the one defined earlier with a couple additional features. First there is a \verb|random_state| parameter that allows you to set the random generator seed and second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (useful if you have a separate DataFrame for labels)


\begin{lstlisting}
  from sklearn.model_selection import train_set_split

  train_set, test_set = train_set_split(data, test_size=0.2, random_state=42)
\end{lstlisting}

There are other kind of splits. For example there's the stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population.

\begin{lstlisting}
  from sklearn.model_selection import StratifiedShuffleSplit

  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
  for train_index, test_index in split.split(df, df['categorized_feature']):
    strat_train_set = df.loc[train_index]
    strat_test_set = df.loc[test_index]
\end{lstlisting}

This split will make sure to keep the same proportions of the categories in \\\lstinline{df['categorized_feature']} in the test set as in the full set.\\

Rq. It can be interesting to use this kind of split even if the feature is not categorized. For example for house price prediction, median income could be an important feature. Since it's continuous you need to create a categorical attribute. For that you can visualize the histogram and create a new feature with \lstinline{pd.cut} that you can remove once the split is done.

\subsection{Discover the Data}

\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude")
\end{lstlisting}

Visualize density:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
\end{lstlisting}

you may need to play around with visualization parameters to make the patterns stand out.\\

You can include the district's population and the prices:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population", figsize=(10,7), c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True)
  plt.legend()
\end{lstlisting}

\subsection{Looking for Correlations}

\begin{lstlisting}
  corr_matrix = df.corr()
  corr_matrix["median_house_value"].sort_values(ascending=False)
\end{lstlisting}

Rq. The correlation coefficient only measures linear correlations. It may completely miss out on nonlinear relationships.\\

Once you have a few promising attributes, you can plot them against each other with Pandas:
\begin{lstlisting}
  from pandas.plotting import scatter_matrix

  attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

  scatter_matrix(housing[attributes], figsize=(12, 8))
\end{lstlisting}

\subsection{Data Cleaning}

Most Machine Learning algorithms cannot work with missing features. You have three options:
\begin{itemize}
  \item [-] Get rid of the correspondig entries
  \item [-] Get rid of the whole attribute
  \item [-] Set the values to some value (zero, the mean, the median...)
\end{itemize}

You can accomplish these easily using \lstinline{dropna(), drop(), fillna()}
\begin{lstlisting}
  df.dropna(subset=["column"])
  df.drop("column", axis=1)
  median = df["column"].median()
  df["column"].fillna(median, inplace=True)
\end{lstlisting}

Scikit-Learn provides a handy class to take care of missing values: \lstinline{SimpleImputer}
\begin{lstlisting}
  from sklearn.impute import SimpleImputer

  imputer = SimpleImputer(strategy="median")
\end{lstlisting}

You need do drop any non numeric attributes and then fit the imputer instance to the training data:
\begin{lstlisting}
  imputer.fit(df)
\end{lstlisting}

Now you can use this ``trained'' imputer to transform the training set:
\begin{lstlisting}
  X = imputer.transform(df)
\end{lstlisting}

The result is a NumPy array that you can put back into a Pandas DataFrame:
\begin{lstlisting}
  df_transformed = pd.DataFrame(X, columns=df.columns)
\end{lstlisting}

\subsection{Handling Text and Categorical Attributes}

Most Machine Learning algorithms prefer to work with numbers. It's possible to convert categories from text to numbers. For this, we can use Scikit-Learn's OrdinalEncoder class:
\begin{lstlisting}
  from sklearn.preprocessing import OrdinalEncoder

  ordinal_encoder = OrdinalEncoder()
  df_encoded = ordinal_encoder.fit_transform(df)
\end{lstlisting}

One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g. for ordered categories such as ``bad'', ``average'' and ``good''). One solution is to use one-hot encoding:
\begin{lstlisting}
  from sklearn.preprocessing import OneHotEncoder

  encoder = OneHotEncoder()
  df_1hot = encoder.fit_transform(df)
\end{lstlisting}

Notice that the output is a SciPy sparse matrix. This is very useful when you have categorical attributes with thousands of categories.

\section{Scikit-learn Design}

All objects share a consistent and simple interface:
\begin{description}
  \item [Estimators] Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator). The estimation itself is performed by the \lstinline{fit()} method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as an imputer’s strategy), and it must be set as an instance variable (generally via a constructor parameter).
  \item [Transformers] Some estimators (such as an imputer) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the \lstinline{transform()} method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called \lstinline{fit_transform()} that is equivalent to calling \lstinline{fit()} and then \lstinline{transform()} (but sometimes \lstinline{fit_transform()} is optimized and runs much faster).
  \item [Predictors] Finally, some estimators are capable of making predictions given a dataset; they are called predictors. For example, the LinearRegression model in the previous chapter was a predictor: it predicted life satisfaction given a country’s GDP per capita. A predictor has a \lstinline{predict()} method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a \lstinline{score()} method that measures the quality of the predictions given a test set (and the corresponding labels in the case of supervised learning
algorithms)
\end{description}


\section{Custom transformers}

All you need is to create a class and implement three methods: \lstinline{fit(), self(), fit_tranform()}\\

You can get the last one for free by symply adding \lstinline{TransformerMixin} as base class. Also if you add \lstinline{BaseEstimator} as a base class you will get two extra methods: \lstinline{get_params(), set_params()}

\subsection{Feature scaling}

With few exceptions, ML algorithms don't perform well when the input numerical attributes have very different scales.\\

There are two common ways to get all attributes to have the same scale.  min-max scaling and standardization. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms. However, standardization is much less affected by outliers.

\subsection{Transformation pipelines}

There are many data transformation steps that need to be executed in the right order. Scikit-learn provides the \lstinline{Pipeline} class to help with such sequences of transformations.

The \lstinline{Pipeline} constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e have the \lstinline{fit_transform} method)

\begin{lstlisting}
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
('imputer', SimpleImputer(strategy="median")),
('attribs_adder', CombinedAttributesAdder()),
('std_scaler', StandardScaler()),
])

housing_num_tr = num_pipeline.fit_transform(housing_num)
\end{lstlisting}

The pipeline exposes the same methods as the final estimator.\\

It's possible to create a pipeline for both numerical and categorical attributes:

\begin{lstlisting}
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num) # numerical columns
cat_attribs = ["ocean_proximity"] # categoric columns

full_pipeline = ColumnTransformer([
("num", num_pipeline, num_attribs),
("cat", OneHotEncoder(), cat_attribs),
])

housing_prepared = full_pipeline.fit_transform(housing)
\end{lstlisting}

Rq. By default the remaining columns (the ones not listed) will be dropped

\section{Select and Train a Model}

\subsection{Evaluate a model}

You don't want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part for model validation.

You can use Scikit-Learn's \lstinline{K-fold cross-validation} feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result in an array containing the 10 evaluation scores.

\begin{lstlisting}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)

tree_rmse_scores = np.sqrt(-scores)
\end{lstlisting}

Rq. Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root

\section{Fine tune your model}

Let's assume you now have a short list of promising models. You now need to fine-tune them.

\subsection{Grid Search}

All you need to do is tell \lstinline{GridSearchCV} which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameters values, using cross-validation.

\subsection{Randomized Search}

The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use \lstinline{RandomizedSearchCV} instead. Instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.

\subsection{Ensemble Methods}

Another way to fine-tune your system is to try to combine the models that perform best. The group (or “ensemble”) will often perform better than the best individual model (just like Random Forests perform better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors.


\subsection{Evaluate your system on the test set}
After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your \lstinline{full_pipeline} to transform the data (call \lstinline{transform()} , not \lstinline{fit_transform()} , you do not want to fit the test set!), and evaluate the final model on the test set

\begin{lstlisting}
final_model = grid_search.best_estimator_

X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
\end{lstlisting}

You might want to have an idea of how precise this estimate is.  For this, you can compute a 95\% confidence interval for the generalization error using scipy.stats.t.interval() :
\begin{lstlisting}
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))
\end{lstlisting}

\section{Classification}

\subsection{Performance measures}

Evaluating a classifier is often significantly trickier than evaluating a regressor. Accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).

\subsubsection{Confusion Matrix}

The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5\textsuperscript{th} row and 3\textsuperscript{rd} column of the confusion matrix.\\

The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive predictions; this is called the precision of the classifier.

\[precision = \frac{TP}{TP + FP}\]

Precision is typically used along with another metric named recall, also called sensitivity or true positive rate.

\[recall = \frac{TP}{TP + FN}\]

It is often convenient to combine precision and recall into a single metric called the F 1 score, in particular if you need a simple way to compare two classifiers. The F 1 score is the harmonic mean of precision and recall.

Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.  As a result, the classifier will only get a high F 1 score if both recall and precision are high.

\[F_1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}}\]

The F\textsubscript{1} score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.

\subsubsection{Precision/Recall Tradeoff}

Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier’s \lstinline{predict()} method, you can call its \lstinline{decision_function()} method, which returns a score for each instance, and then make predictions based on those scores using any threshold you wan

\subsection{The ROC Curve}

The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive.\\

Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise.

\subsection{Multiclass Classification}

Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification using multiple binary classifiers.

\begin{enumerate}
    \item For example, one way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest).
    \item nother strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy.
\end{enumerate}

Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO)

\subsection{Multilabel Classification}

Until now each instance has always been assigned to just one class. In some cases you may want your classifier to output multiple classes for each instance.

\subsection{Multioutput Classification}

It is simply a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).

\section{Training Models}

\subsection{Linear Regression Model}

A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term.
\[\hat y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n = h_{\bm{\theta}}(\bm{x}) = \bm{\theta^T} \bm{x}\]
Training a model means setting its parameters so that the model best fits the training set. The most common performance measure of a regression model is the Root Mean Square Error (RMSE). It is simpler to minimize the Mean Square Error (MSE) Linear Regression than the RMSE, and it leads to the same result.
\[ MSE(\bm{X}, h_{\bm{\theta}}) = \frac{1}{m} \sum_{i=1}^m (\bm{\theta}^T\bm{x}^{(i)} - \bm{y}^{(i)}  )^2\]

\subsubsection{The Normal Equation}
To find the value of $\bm{\theta}$ that minimizes the cost function, there is a closed-form solution in other words, a mathematical equation that gives the result directly. This is called the Normal Equation:
\[\hat \theta = (\bm{X}^T \bm{X})^{-1}\bm{X}^T \bm{y}\]

\subsection{Gradient Descent}

Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. It measures the local gradient of the error function with regards to the parameter vector $\theta$, and it goes in the direction of descending gradient.\\

Concretely, you start by filling $\bm{\theta}$ with random values (this is called random initialization), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum. \\

An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter.

\subsubsection{Batch Gradient Descent}

To implement Gradient Descent, you need to compute the gradient of the cost function with regards to each model parameter $\theta_j$.\\

Instead of computing these partial derivatives individually, you can use The gradient vector, noted $\nabla_{\bm{\theta}}MSE(\bm{\theta})$ which contains all the
partial derivatives of the cost function (one for each model parameter).

\[\nabla_{\bm{\theta}}MSE(\bm{\theta}) = \frac{2}{m} \bm{X}^T(\bm{X \theta - y}) \]
Notice that this formula involves calculations over the full training set $\bm{X}$, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step (actually, Full Gradient Descent would probably be a better name).

As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features.\\

Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\nabla_{\bm{\theta}}MSE(\bm{\theta})$ from $\bm{\theta}$. This is where the learning rate $\eta$ comes into play: multiply the gradient vector by $\eta$ to determine the size of the downhill step.
\[\bm{\theta}^{(next\ step)} = \bm{\theta} - \eta \nabla_{\bm{\theta}}MSE(\bm{\theta})\]

\begin{lstlisting}
eta = 0.1
n_iterations = 1000
m = 100

theta = np.random.randn(2,1)

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
\end{lstlisting}

The result is pretty much the same as with the Normal Equation!

\subsubsection{Stochastic Gradient Descent}

The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance.\\

This algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down

\begin{lstlisting}
n_epochs = 50
t_0, t_1 = 5, 50

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    for i in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
\end{lstlisting}

\subsubsection{Mini-batch Gradient Descent}

At each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Minibatch GD computes the gradients on small random sets of instances called minibatches.\\

\subsubsection{Comparison of algorithms for Linear Regression}

\begin{tabular}{@{}llllll@{}}
    \toprule
    Algorithm & Large m & Large n & Hyperparams & Scalling required & Scikit-Learn \\
    \hline
    Normal Equation & Fast & Slow & 0 & No & n/a \\
    SVD & Fast & Slow & 0 & No & LinearRegression \\
    Batch GD & Slow & Fast & 2 & Yes & SGDRegressor \\
    Stochastic GD & Fast & Fast & $\geq 2$ & Yes & SGDRegressor \\
    Mini-batch GD & Fast & Fast & $\geq 2$ & Yes & SGDRegressor \\
    \bottomrule
\end{tabular}

\subsection{Polynomial Regression}

What if your data is actually more complex than a simple straight line? Surprisingly, you can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.

\subsection{Learning Curves}

How can you tell that your model is overfitting or underfitting the data?

You can use cross-validation to get an estimate of a model’s generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.\\

Another way is to look at the learning curves: these are plots of the model’s performance on the training set and the validation set as a function of the training set size (or the training iteration).\\

If both curves have reached a plateau, they are close and fairly high, these learning curves are typical of an underfitting model. If your model is underfitting the training data, adding more training examples will not help. You need to use a more complex model or come up with better features.\\

If however there is a gap between the curves, this means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer. One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error.

\subsubsection{The Bias / Variance Tradeoff}

An important theoretical result of statistics and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of three very different errors:

\begin{description}
    \item [Bias]: This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.
    \item [Variance]: This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data.
    \item [Irreducible Error]: This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).
\end{description}

Increasing a model’s complexity will typically increase its variance and reduce its bias.  Conversely, reducing a model’s complexity increases its bias and reduces its variance.  This is why it is called a tradeoff.

\subsection{Regularized Linear Models}

A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees.\\

For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights.

\subsubsection{Ridge Regression}

A regularization term equal to $\alpha \sum_{i=1}^n \theta_i^2$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible.\\

Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.

It is quite common for the cost function used during training to be different from the performance measure used for testing. Apart from regularization, another reason why they might be different is that a good training cost function should have optimization-friendly derivatives, while the performance measure used for testing should be as close as possible to the final objective.\\

The hyperparameter $\alpha$ controls how much you want to regularize the model. If $\alpha$ = 0 then Ridge Regression is just Linear Regression. If $\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean.

Ridge Regression cost function:
\[J(\bm{\theta}) = MSE(\bm{\theta}) + \alpha \sum_{i=1}^n \theta_i^2\]
Note that the bias term $\theta_0$ is not regularized.\\

As with Linear Regression, we can perform Ridge Regression either by computing a closed-form equation or by performing Gradient Descent. The pros and cons are the same.

\subsubsection{Lasso Regression}

Just like Ridge Regression, it adds a regularization term to the cost function, but it uses the $l_1$ norm of the weight vector instead of half the square of the $l_2$ norm.

Lasso Regression cost function:
\[J(\bm{\theta}) = MSE(\bm{\theta}) + \alpha \sum_{i=1}^n |\theta_i|\]

An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).

\subsubsection{Elastic Net}

Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression.

Elastic Net cost function:
\[J(\bm{\theta}) = MSE(\bm{\theta}) + r \alpha \sum_{i=1}^n |\theta_i| + \frac{1-r}{2} \alpha \sum_{i=1}^n \theta_i^2\]

So when should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.

\subsubsection{Early Stopping}

A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping.

\subsection{Logistic Regression}

some regression algorithms can be used for classification as well (and vice versa). Logistic Regression (also called Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class.\\

So how does it work? Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the logistic of this result.\\

The logistic noted $\sigma(.)$ is a sigmoid function that outputs a number between 0 and 1.
\[\sigma(t) = \frac{1}{1+\exp{(-t)}}\]

Once the Logistic Regression model has estimated the probability $\hat p = h_{\theta}(\bm{x})$ that an instance belongs to the positive class, it can make its prediction $\hat y$ easily.

\subsubsection{Training and Cost Function}
\[
    c(\bm{\theta}) =
    \begin{cases}
        -\log{(\hat p)} & \text{if $y=1$}\\
        -\log{(1 - \hat p)} & \text{if $y=0$}
    \end{cases}
\]
The cost function over the whole training set is simply the average cost over all training instances.\\
Logistic Regression cost function:
\[J(\bm{\theta}) = -\frac{1}{m} \sum_{i=1}^m [y^{(i)}\log(\hat p^{(i)}) + (1-y^{(i)})\log(1- \hat p^{(i)})]\]

The bad news is that there is no known closed-form equation to compute the value of $\bm{\theta}$ that minimizes this cost function (there is no equivalent of the Normal Equation).  But the good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough).

\subsubsection{Softmax Regression}

The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.\\

The idea is quite simple: when given an instance $\bm{x}$, the Softmax Regression model first computes a score $s_k(\bm{x})$ for each class k, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores.\\

Note that each class has its own dedicated parameter vector $\bm{\theta}(k)$. All these vectors are typically stored as rows in a parameter matrix $\bm{\Theta}$.

Cross entropy cost function:
\[J(\bm{\Theta}) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log(\hat p_k^{(i)})\]

\section{Support Vector Machines}

\end{document}
