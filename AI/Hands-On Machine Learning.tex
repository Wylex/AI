\documentclass[french]{article}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}

\title{Hands-On Machine Learning}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  frame=leftline,
  rulecolor=\color{gray},
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a }}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}

\begin{document}
\date{}

\maketitle

\setlength{\parindent}{0cm}

\section{Extract the data}

Get a quick description of the data (number of rows and each attribute's type):
\begin{lstlisting}
  df.info()
\end{lstlisting}

Find out what categories exist:
\begin{lstlisting}
  df['c'].value_counts()
\end{lstlisting}

Summary of the numerical attributes:
\begin{lstlisting}
  df.describe()
\end{lstlisting}

Plot histogram for each numerical attribute:
\begin{lstlisting}
  %matplotlib inline
  import matplotlib.pyplot as plt
  df.hist(bins=50)
  plt.show()
\end{lstlisting}
Rq. Calling \verb|show()| in a Jupyter notebook is optional


\subsection{Test set}

Create a test set:
\begin{lstlisting}
  def split_train_set(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffle_indices[test_set_size:]

    return data.iloc[train_indices], data.iloc[test_indices]
\end{lstlisting}

Rq. If the program runs again, the test set will not be the same which is a problem.\\
One solution is to save the test set. Another option is to set the random number generator's seed. Both solutions will break next time you fetch an updated dataset. A common solution is to use each instance's identifier (unique and immutable) to decide whether or not it should go in the test set. For example, you could compute a hash of each instance's identifier and put that instance in the test set if the hash is lower or equal to 20\%
\begin{lstlisting}
  from zlib import crc32

  def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

  def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))

    return data.loc[~in_test_set], data.loc[in_test_set]
\end{lstlisting}

Rq. If you use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset, and no row ever gets deleted. If this is not possible, then you can try to use the most stable featues to build a unique identifier.\\

Scikit-learn provides a few function to split datasets. \\The simplest is \verb|split_train_test|, which does pretty much the same thing as the one defined earlier with a couple additional features. First there is a \verb|random_state| parameter that allows you to set the random generator seed and second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (useful if you have a separate DataFrame for labels)


\begin{lstlisting}
  from sklearn.model_selection import train_set_split

  train_set, test_set = train_set_split(data, test_size=0.2, random_state=42)
\end{lstlisting}

There are other kind of splits. For example there's the stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population.

\begin{lstlisting}
  from sklearn.model_selection import StratifiedShuffleSplit

  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
  for train_index, test_index in split.split(df, df['categorized_feature']):
    strat_train_set = df.loc[train_index]
    strat_test_set = df.loc[test_index]
\end{lstlisting}

This split will make sure to keep the same proportions of the categories in \\\lstinline{df['categorized_feature']} in the test set as in the full set.\\

Rq. It can be interesting to use this kind of split even if the feature is not categorized. For example for house price prediction, median income could be an important feature. Since it's continuous you need to create a categorical attribute. For that you can visualize the histogram and create a new feature with \lstinline{pd.cut} that you can remove once the split is done.

\subsection{Discover the Data}

\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude")
\end{lstlisting}

Visualize density:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
\end{lstlisting}

you may need to play around with visualization parameters to make the patterns stand out.\\

You can include the district's population and the prices:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population", figsize=(10,7), c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True)
  plt.legend()
\end{lstlisting}

\subsection{Looking for Correlations}

\begin{lstlisting}
  corr_matrix = df.corr()
  corr_matrix["median_house_value"].sort_values(ascending=False)
\end{lstlisting}

Rq. The correlation coefficient only measures linear correlations. It may completely miss out on nonlinear relationships.\\

Once you have a few promising attributes, you can plot them against each other with Pandas:
\begin{lstlisting}
  from pandas.plotting import scatter_matrix

  attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

  scatter_matrix(housing[attributes], figsize=(12, 8))
\end{lstlisting}

\end{document}
