\documentclass[french]{article}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{bm}

\title{Hands-On Machine Learning}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  frame=leftline,
  rulecolor=\color{gray},
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a }}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}

\begin{document}
\date{}

\maketitle

\setlength{\parindent}{0cm}

\section{Extract the data}

Get a quick description of the data (number of rows and each attribute's type):
\begin{lstlisting}
  df.info()
\end{lstlisting}

Find out what categories exist:
\begin{lstlisting}
  df['c'].value_counts()
\end{lstlisting}

Summary of the numerical attributes:
\begin{lstlisting}
  df.describe()
\end{lstlisting}

Plot histogram for each numerical attribute:
\begin{lstlisting}
  %matplotlib inline
  import matplotlib.pyplot as plt
  df.hist(bins=50)
  plt.show()
\end{lstlisting}
Rq. Calling \verb|show()| in a Jupyter notebook is optional


\subsection{Test set}

Create a test set:
\begin{lstlisting}
  def split_train_set(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffle_indices[test_set_size:]

    return data.iloc[train_indices], data.iloc[test_indices]
\end{lstlisting}

Rq. If the program runs again, the test set will not be the same which is a problem.\\
One solution is to save the test set. Another option is to set the random number generator's seed. Both solutions will break next time you fetch an updated dataset. A common solution is to use each instance's identifier (unique and immutable) to decide whether or not it should go in the test set. For example, you could compute a hash of each instance's identifier and put that instance in the test set if the hash is lower or equal to 20\%
\begin{lstlisting}
  from zlib import crc32

  def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

  def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))

    return data.loc[~in_test_set], data.loc[in_test_set]
\end{lstlisting}

Rq. If you use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset, and no row ever gets deleted. If this is not possible, then you can try to use the most stable featues to build a unique identifier.\\

Scikit-learn provides a few function to split datasets. \\The simplest is \verb|split_train_test|, which does pretty much the same thing as the one defined earlier with a couple additional features. First there is a \verb|random_state| parameter that allows you to set the random generator seed and second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (useful if you have a separate DataFrame for labels)


\begin{lstlisting}
  from sklearn.model_selection import train_set_split

  train_set, test_set = train_set_split(data, test_size=0.2, random_state=42)
\end{lstlisting}

There are other kind of splits. For example there's the stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population.

\begin{lstlisting}
  from sklearn.model_selection import StratifiedShuffleSplit

  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
  for train_index, test_index in split.split(df, df['categorized_feature']):
    strat_train_set = df.loc[train_index]
    strat_test_set = df.loc[test_index]
\end{lstlisting}

This split will make sure to keep the same proportions of the categories in \\\lstinline{df['categorized_feature']} in the test set as in the full set.\\

Rq. It can be interesting to use this kind of split even if the feature is not categorized. For example for house price prediction, median income could be an important feature. Since it's continuous you need to create a categorical attribute. For that you can visualize the histogram and create a new feature with \lstinline{pd.cut} that you can remove once the split is done.

\subsection{Discover the Data}

\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude")
\end{lstlisting}

Visualize density:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
\end{lstlisting}

you may need to play around with visualization parameters to make the patterns stand out.\\

You can include the district's population and the prices:
\begin{lstlisting}
  housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population", figsize=(10,7), c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True)
  plt.legend()
\end{lstlisting}

\subsection{Looking for Correlations}

\begin{lstlisting}
  corr_matrix = df.corr()
  corr_matrix["median_house_value"].sort_values(ascending=False)
\end{lstlisting}

Rq. The correlation coefficient only measures linear correlations. It may completely miss out on nonlinear relationships.\\

Once you have a few promising attributes, you can plot them against each other with Pandas:
\begin{lstlisting}
  from pandas.plotting import scatter_matrix

  attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

  scatter_matrix(housing[attributes], figsize=(12, 8))
\end{lstlisting}

\subsection{Data Cleaning}

Most Machine Learning algorithms cannot work with missing features. You have three options:
\begin{itemize}
  \item [-] Get rid of the correspondig entries
  \item [-] Get rid of the whole attribute
  \item [-] Set the values to some value (zero, the mean, the median...)
\end{itemize}

You can accomplish these easily using \lstinline{dropna(), drop(), fillna()}
\begin{lstlisting}
  df.dropna(subset=["column"])
  df.drop("column", axis=1)
  median = df["column"].median()
  df["column"].fillna(median, inplace=True)
\end{lstlisting}

Scikit-Learn provides a handy class to take care of missing values: \lstinline{SimpleImputer}
\begin{lstlisting}
  from sklearn.impute import SimpleImputer

  imputer = SimpleImputer(strategy="median")
\end{lstlisting}

You need do drop any non numeric attributes and then fit the imputer instance to the training data:
\begin{lstlisting}
  imputer.fit(df)
\end{lstlisting}

Now you can use this ``trained'' imputer to transform the training set:
\begin{lstlisting}
  X = imputer.transform(df)
\end{lstlisting}

The result is a NumPy array that you can put back into a Pandas DataFrame:
\begin{lstlisting}
  df_transformed = pd.DataFrame(X, columns=df.columns)
\end{lstlisting}

\subsection{Handling Text and Categorical Attributes}

Most Machine Learning algorithms prefer to work with numbers. It's possible to convert categories from text to numbers. For this, we can use Scikit-Learn's OrdinalEncoder class:
\begin{lstlisting}
  from sklearn.preprocessing import OrdinalEncoder

  ordinal_encoder = OrdinalEncoder()
  df_encoded = ordinal_encoder.fit_transform(df)
\end{lstlisting}

One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g. for ordered categories such as ``bad'', ``average'' and ``good''). One solution is to use one-hot encoding:
\begin{lstlisting}
  from sklearn.preprocessing import OneHotEncoder

  encoder = OneHotEncoder()
  df_1hot = encoder.fit_transform(df) 
\end{lstlisting}

Notice that the output is a SciPy sparse matrix. This is very useful when you have categorical attributes with thousands of categories.

\section{Scikit-learn Design}

All objects share a consistent and simple interface:
\begin{description}
  \item [Estimators] Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator). The estimation itself is performed by the \lstinline{fit()} method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as an imputer’s strategy), and it must be set as an instance variable (generally via a constructor parameter).
  \item [Transformers] Some estimators (such as an imputer) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the \lstinline{transform()} method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called \lstinline{fit_transform()} that is equivalent to calling \lstinline{fit()} and then \lstinline{transform()} (but sometimes \lstinline{fit_transform()} is optimized and runs much faster).
  \item [Predictors] Finally, some estimators are capable of making predictions given a dataset; they are called predictors. For example, the LinearRegression model in the previous chapter was a predictor: it predicted life satisfaction given a country’s GDP per capita. A predictor has a \lstinline{predict()} method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a \lstinline{score()} method that measures the quality of the predictions given a test set (and the corresponding labels in the case of supervised learning
algorithms)
\end{description}


\section{Custom transformers}

All you need is to create a class and implement three methods: \lstinline{fit(), self(), fit_tranform()}\\

You can get the last one for free by symply adding \lstinline{TransformerMixin} as base class. Also if you add \lstinline{BaseEstimator} as a base class you will get two extra methods: \lstinline{get_params(), set_params()}

\subsection{Feature scaling}

With few exceptions, ML algorithms don't perform well when the input numerical attributes have very different scales.\\

There are two common ways to get all attributes to have the same scale.  min-max scaling and standardization. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms. However, standardization is much less affected by outliers.

\subsection{Transformation pipelines}

There are many data transformation steps that need to be executed in the right order. Scikit-learn provides the \lstinline{Pipeline} class to help with such sequences of transformations.

The \lstinline{Pipeline} constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e have the \lstinline{fit_transform} method)

\begin{lstlisting}
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
('imputer', SimpleImputer(strategy="median")),
('attribs_adder', CombinedAttributesAdder()),
('std_scaler', StandardScaler()),
])

housing_num_tr = num_pipeline.fit_transform(housing_num)
\end{lstlisting}

The pipeline exposes the same methods as the final estimator.\\

It's possible to create a pipeline for both numerical and categorical attributes:

\begin{lstlisting}
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num) # numerical columns
cat_attribs = ["ocean_proximity"] # categoric columns

full_pipeline = ColumnTransformer([
("num", num_pipeline, num_attribs),
("cat", OneHotEncoder(), cat_attribs),
])

housing_prepared = full_pipeline.fit_transform(housing)
\end{lstlisting}

Rq. By default the remaining columns (the ones not listed) will be dropped

\section{Select and Train a Model}

\subsection{Evaluate a model}

You don't want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part for model validation.

You can use Scikit-Learn's \lstinline{K-fold cross-validation} feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result in an array containing the 10 evaluation scores.

\begin{lstlisting}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)

tree_rmse_scores = np.sqrt(-scores)
\end{lstlisting}

Rq. Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root 

\section{Fine tune your model}

Let's assume you now have a short list of promising models. You now need to fine-tune them.

\subsection{Grid Search}

All you need to do is tell \lstinline{GridSearchCV} which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameters values, using cross-validation.

\subsection{Randomized Search}

The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use \lstinline{RandomizedSearchCV} instead. Instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.

\subsection{Ensemble Methods}

Another way to fine-tune your system is to try to combine the models that perform best. The group (or “ensemble”) will often perform better than the best individual model (just like Random Forests perform better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors.  


\subsection{Evaluate your system on the test set}
After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your \lstinline{full_pipeline} to transform the data (call \lstinline{transform()} , not \lstinline{fit_transform()} , you do not want to fit the test set!), and evaluate the final model on the test set

\begin{lstlisting}
final_model = grid_search.best_estimator_

X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
\end{lstlisting}

You might want to have an idea of how precise this estimate is.  For this, you can compute a 95\% confidence interval for the generalization error using scipy.stats.t.interval() :
\begin{lstlisting}
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))
\end{lstlisting}

\section{Classification}

\subsection{Performance measures}

Evaluating a classifier is often significantly trickier than evaluating a regressor. Accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).

\subsubsection{Confusion Matrix}

The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5\textsuperscript{th} row and 3\textsuperscript{rd} column of the confusion matrix.\\

The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive predictions; this is called the precision of the classifier.

\[precision = \frac{TP}{TP + FP}\]

Precision is typically used along with another metric named recall, also called sensitivity or true positive rate.

\[recall = \frac{TP}{TP + FN}\]

It is often convenient to combine precision and recall into a single metric called the F 1 score, in particular if you need a simple way to compare two classifiers. The F 1 score is the harmonic mean of precision and recall.

Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.  As a result, the classifier will only get a high F 1 score if both recall and precision are high.

\[F_1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}}\]

The F\textsubscript{1} score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.

\subsubsection{Precision/Recall Tradeoff}

Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier’s \lstinline{predict()} method, you can call its \lstinline{decision_function()} method, which returns a score for each instance, and then make predictions based on those scores using any threshold you wan

\subsection{The ROC Curve}

The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive.\\

Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise.

\subsection{Multiclass Classification}

Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification using multiple binary classifiers.

\begin{enumerate}
    \item For example, one way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest).
    \item nother strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy.
\end{enumerate}

Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO) 

\subsection{Multilabel Classification}

Until now each instance has always been assigned to just one class. In some cases you may want your classifier to output multiple classes for each instance.

\subsection{Multioutput Classification}

It is simply a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).

\section{Training Models}

\subsection{Linear Regression Model}

A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term.
\[\hat y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n = h_{\bm{\theta}}(\bm{x}) = \bm{\theta^T} \bm{x}\]
Training a model means setting its parameters so that the model best fits the training set. The most common performance measure of a regression model is the Root Mean Square Error (RMSE). It is simpler to minimize the Mean Square Error (MSE) Linear Regression than the RMSE, and it leads to the same result.
\[ MSE(\bm{X}, h_{\bm{\theta}}) = \frac{1}{m} \sum_{i=1}^m (\bm{\theta}^T\bm{x}^{(i)} - \bm{y}^{(i)}  )^2\]

\subsubsection{The Normal Equation}
To find the value of $\bm{\theta}$ that minimizes the cost function, there is a closed-form solution in other words, a mathematical equation that gives the result directly. This is called the Normal Equation:
\[\hat \theta = (\bm{X}^T \bm{X})^{-1}\bm{X}^T \bm{y}\]

\subsection{Gradient Descent}

Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. It measures the local gradient of the error function with regards to the parameter vector $\theta$, and it goes in the direction of descending gradient.\\

Concretely, you start by filling $\bm{\theta}$ with random values (this is called random initialization), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum. \\

An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter.

\subsubsection{Batch Gradient Descent}

To implement Gradient Descent, you need to compute the gradient of the cost function with regards to each model parameter $\theta_j$.\\

Instead of computing these partial derivatives individually, you can use The gradient vector, noted $\nabla_{\bm{\theta}}MSE(\bm{\theta})$ which contains all the
partial derivatives of the cost function (one for each model parameter).

\[\nabla_{\bm{\theta}}MSE(\bm{\theta}) = \frac{2}{m} \bm{X}^T(\bm{X \theta - y}) \]
Notice that this formula involves calculations over the full training set $\bm{X}$, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step (actually, Full Gradient Descent would probably be a better name).

As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features.\\

Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\nabla_{\bm{\theta}}MSE(\bm{\theta})$ from $\bm{\theta}$. This is where the learning rate $\eta$ comes into play: multiply the gradient vector by $\eta$ to determine the size of the downhill step.
\[\bm{\theta}^{(next\ step)} = \bm{\theta} - \eta \nabla_{\bm{\theta}}MSE(\bm{\theta})\]

\begin{lstlisting}
eta = 0.1
n_iterations = 1000
m = 100

theta = np.random.randn(2,1)

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
\end{lstlisting}

The result is pretty much the same as with the Normal Equation!

\subsubsection{Stochastic Gradient Descent}

The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance.\\

This algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down

\begin{lstlisting}
n_epochs = 50
t_0, t_1 = 5, 50

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    for i in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
\end{lstlisting}


\end{document}
